#!/usr/bin/env python3
"""
Image Analysis Tool for Software Anomaly Detection Visualizations

This script analyzes and displays all the PNG files generated by the anomaly detection models
with descriptions and insights for better understanding.
"""

import os
import matplotlib.pyplot as plt
import matplotlib.image as mpimg
from PIL import Image
import pandas as pd
import numpy as np

def analyze_images():
    """Analyze all PNG files in the current directory"""
    
    print("=" * 80)
    print("SOFTWARE ANOMALY DETECTION - IMAGE ANALYSIS TOOL")
    print("=" * 80)
    
    # Get all PNG files from plots directory
    plots_dir = '../output/plots'
    png_files = [f for f in os.listdir(plots_dir) if f.endswith('.png')]
    png_files = [os.path.join(plots_dir, f) for f in png_files]
    png_files.sort()
    
    print(f"\n📊 Found {len(png_files)} visualization files:")
    for i, file in enumerate(png_files, 1):
        print(f"   {i:2d}. {file}")
    
    # Categorize images by model
    categories = {
        'Isolation Forest': [f for f in png_files if 'isolation_forest' in f],
        'One-Class SVM': [f for f in png_files if 'oneclass_svm' in f],
        'Local Outlier Factor': [f for f in png_files if 'lof' in f],
        'Autoencoder': [f for f in png_files if 'autoencoder' in f],
        'Summary & Analysis': [f for f in png_files if any(x in f for x in ['high_risk', 'model_comparison', 'flag_distribution', 'comprehensive_analysis'])]
    }
    
    print(f"\n📁 Images by Category:")
    for category, files in categories.items():
        if files:
            print(f"\n   🎯 {category} ({len(files)} files):")
            for file in files:
                print(f"      • {file}")
    
    # Image descriptions and insights
    image_descriptions = {
        'isolation_forest_scatter.png': {
            'title': 'Isolation Forest - Scatter Plot',
            'description': 'Shows Publisher vs Product Name with anomaly detection. Red points are anomalies, blue are normal.',
            'insights': 'Look for clusters of red points - these indicate suspicious software patterns.'
        },
        'isolation_forest_counts.png': {
            'title': 'Isolation Forest - Anomaly Counts',
            'description': 'Bar chart showing the count of normal vs anomalous records detected by Isolation Forest.',
            'insights': 'Shows the 5% contamination rate - approximately 176 anomalies detected.'
        },
        'isolation_forest_publishers.png': {
            'title': 'Isolation Forest - Top Publishers',
            'description': 'Top 10 publishers with the most anomalies detected by Isolation Forest.',
            'insights': 'Identifies which software publishers have the most suspicious installations.'
        },
        'oneclass_svm_scatter.png': {
            'title': 'One-Class SVM - Scatter Plot',
            'description': 'Publisher vs Product Name scatter plot with SVM anomaly detection.',
            'insights': 'SVM creates a boundary around normal data - points outside are anomalies.'
        },
        'oneclass_svm_counts.png': {
            'title': 'One-Class SVM - Anomaly Counts',
            'description': 'Count of normal vs anomalous records from One-Class SVM.',
            'insights': 'Shows 172 anomalies detected (4.91% of data).'
        },
        'oneclass_svm_publishers.png': {
            'title': 'One-Class SVM - Top Publishers',
            'description': 'Top publishers with most anomalies from SVM analysis.',
            'insights': 'Compare with Isolation Forest results to see consistency across models.'
        },
        'oneclass_svm_scores.png': {
            'title': 'One-Class SVM - Decision Scores',
            'description': 'Distribution of SVM decision scores and scores by class.',
            'insights': 'Shows how well the model separates normal from anomalous data.'
        },
        'lof_scatter.png': {
            'title': 'Local Outlier Factor - Scatter Plot',
            'description': 'Publisher vs Product Name with LOF anomaly detection.',
            'insights': 'LOF identifies local outliers based on neighborhood density.'
        },
        'lof_counts.png': {
            'title': 'Local Outlier Factor - Anomaly Counts',
            'description': 'Count of normal vs anomalous records from LOF.',
            'insights': 'Shows 173 anomalies detected by LOF algorithm.'
        },
        'lof_publishers.png': {
            'title': 'Local Outlier Factor - Top Publishers',
            'description': 'Top publishers with most anomalies from LOF analysis.',
            'insights': 'LOF is sensitive to local density variations in the data.'
        },
        'lof_scores.png': {
            'title': 'Local Outlier Factor - Score Analysis',
            'description': 'LOF score distributions and analysis by class.',
            'insights': 'Lower LOF scores indicate higher outlier probability.'
        },
        'autoencoder_scatter.png': {
            'title': 'Autoencoder - Scatter Plot',
            'description': 'Publisher vs Product Name with Autoencoder anomaly detection.',
            'insights': 'Autoencoder learns normal patterns and flags reconstruction errors as anomalies.'
        },
        'autoencoder_counts.png': {
            'title': 'Autoencoder - Anomaly Counts',
            'description': 'Count of normal vs anomalous records from Autoencoder.',
            'insights': 'Shows 174 anomalies detected based on reconstruction error.'
        },
        'autoencoder_training.png': {
            'title': 'Autoencoder - Training History',
            'description': 'Training and validation loss over epochs.',
            'insights': 'Shows model convergence and potential overfitting.'
        },
        'autoencoder_analysis.png': {
            'title': 'Autoencoder - Comprehensive Analysis',
            'description': 'MSE distribution, scores by class, and cumulative analysis.',
            'insights': 'Shows reconstruction error distribution and threshold selection.'
        },
        'high_risk_publishers.png': {
            'title': 'High-Risk Publishers Analysis',
            'description': 'Top publishers with software flagged by ≥3 models.',
            'insights': 'Identifies the most suspicious software publishers across all models.'
        },
        'model_comparison.png': {
            'title': 'Model Comparison',
            'description': 'Comparison of anomaly counts across all four models.',
            'insights': 'Shows consistency and differences between detection methods.'
        },
        'flag_distribution.png': {
            'title': 'Flag Distribution Analysis',
            'description': 'Distribution of records by number of models flagging them.',
            'insights': 'Shows how many records are flagged by multiple models.'
        },
        'comprehensive_analysis.png': {
            'title': 'Comprehensive Analysis Dashboard',
            'description': 'Multi-panel analysis including overlap, correlations, and risk levels.',
            'insights': 'Complete overview of all anomaly detection results and patterns.'
        }
    }
    
    print(f"\n📋 Detailed Image Analysis:")
    print("=" * 80)
    
    for file in png_files:
        if file in image_descriptions:
            desc = image_descriptions[file]
            print(f"\n🖼️  {desc['title']}")
            print(f"   📁 File: {file}")
            print(f"   📝 Description: {desc['description']}")
            print(f"   💡 Key Insights: {desc['insights']}")
            
            # Get file size
            size = os.path.getsize(file)
            print(f"   📏 File Size: {size:,} bytes")
        else:
            print(f"\n🖼️  {file}")
            print(f"   📁 File: {file}")
            print(f"   📏 File Size: {os.path.getsize(file):,} bytes")
    
    # Summary statistics
    print(f"\n📊 Summary Statistics:")
    print("=" * 80)
    
    total_size = sum(os.path.getsize(f) for f in png_files)
    print(f"   Total files: {len(png_files)}")
    print(f"   Total size: {total_size:,} bytes ({total_size/1024/1024:.2f} MB)")
    print(f"   Average size: {total_size/len(png_files):,.0f} bytes per file")
    
    # Recommendations for viewing
    print(f"\n💡 Recommendations for Better Understanding:")
    print("=" * 80)
    print("   1. 🖼️  Use an image viewer that supports zoom and pan")
    print("   2. 📊 Compare scatter plots across models to see consistency")
    print("   3. 📈 Focus on high-risk publishers and products")
    print("   4. 🔍 Look for patterns in anomaly distributions")
    print("   5. 📋 Use the comprehensive analysis dashboard for overview")
    print("   6. 🎯 Pay attention to software flagged by multiple models")
    
    return png_files, categories

def create_image_grid():
    """Create a grid view of all images"""
    png_files = [f for f in os.listdir('.') if f.endswith('.png')]
    png_files.sort()
    
    # Calculate grid dimensions
    n_images = len(png_files)
    cols = 4
    rows = (n_images + cols - 1) // cols
    
    fig, axes = plt.subplots(rows, cols, figsize=(20, 5*rows))
    fig.suptitle('Software Anomaly Detection - All Visualizations', fontsize=16, fontweight='bold')
    
    for i, file in enumerate(png_files):
        row = i // cols
        col = i % cols
        
        if rows == 1:
            ax = axes[col] if cols > 1 else axes
        else:
            ax = axes[row, col] if cols > 1 else axes[row]
        
        try:
            img = mpimg.imread(file)
            ax.imshow(img)
            ax.set_title(file.replace('.png', '').replace('_', ' ').title(), fontsize=8)
            ax.axis('off')
        except Exception as e:
            ax.text(0.5, 0.5, f'Error loading\n{file}', ha='center', va='center')
            ax.set_title(file, fontsize=8)
            ax.axis('off')
    
    # Hide empty subplots
    for i in range(n_images, rows * cols):
        row = i // cols
        col = i % cols
        if rows == 1:
            axes[col].axis('off') if cols > 1 else axes.axis('off')
        else:
            axes[row, col].axis('off') if cols > 1 else axes[row].axis('off')
    
    plt.tight_layout()
    plt.savefig('all_visualizations_grid.png', dpi=300, bbox_inches='tight')
    print(f"\n✅ Created overview grid: 'all_visualizations_grid.png'")
    plt.show()

if __name__ == "__main__":
    # Analyze all images
    png_files, categories = analyze_images()
    
    # Create overview grid
    print(f"\n🖼️  Creating overview grid of all visualizations...")
    create_image_grid()
    
    print(f"\n" + "=" * 80)
    print("IMAGE ANALYSIS COMPLETED!")
    print("=" * 80) 